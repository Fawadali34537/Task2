{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy librosa tensorflow scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIuedh1r4ZVx",
        "outputId": "c56d1d22-7a04-4a05-bdcf-f09f36706c68"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.3)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def extract_features(file_name):\n",
        "    try:\n",
        "        audio, sample_rate = librosa.load(file_name, sr=None, res_type='kaiser_fast')\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "        mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
        "        return mfccs_scaled\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define the base directory containing actor folders\n",
        "base_dir = '/content/drive/MyDrive/RAVIDASS'\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Iterate over each actor folder\n",
        "for actor_folder in os.listdir(base_dir):\n",
        "    actor_path = os.path.join(base_dir, actor_folder)\n",
        "\n",
        "    # Check if it's a directory\n",
        "    if os.path.isdir(actor_path):\n",
        "        for filename in os.listdir(actor_path):\n",
        "            if filename.endswith('.wav'):\n",
        "                file_path = os.path.join(actor_path, filename)\n",
        "                features = extract_features(file_path)\n",
        "                if features is not None:\n",
        "                    X.append(features)\n",
        "                    # Extract emotion label from the filename, assuming it follows a pattern\n",
        "                    # Example filename: 03-01-01-01-01-01-01.wav\n",
        "                    label = int(filename.split('-')[2]) - 1\n",
        "                    y.append(label)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Check if data is loaded\n",
        "print(f\"Number of samples: {len(X)}\")\n",
        "if len(X) > 0:\n",
        "    print(f\"Sample feature shape: {X[0].shape}\")\n",
        "    print(f\"Sample label: {y[0]}\")\n",
        "else:\n",
        "    print(\"No data available.\")\n",
        "\n",
        "# Split data into training and validation sets\n",
        "if len(X) > 0:\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    num_classes = 8\n",
        "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "    y_val = to_categorical(y_val, num_classes=num_classes)\n",
        "else:\n",
        "    print(\"Cannot proceed with training as no data is available.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADWQ_FWe4tia",
        "outputId": "d245c6aa-7d73-4264-f9ac-baca66052c1e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 1440\n",
            "Sample feature shape: (40,)\n",
            "Sample label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(os.listdir(data_dir))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFSWTWxQ45M-",
        "outputId": "248e11c7-4843-432e-d6ae-00a5299ebf3f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['audio_speech_actors_01-24', 'Actor_23', 'Actor_24', 'Actor_22', 'Actor_21', 'Actor_18', 'Actor_19', 'Actor_20', 'Actor_17', 'Actor_16', 'Actor_14', 'Actor_12', 'Actor_13', 'Actor_15', 'Actor_11', 'Actor_10', 'Actor_05', 'Actor_09', 'Actor_08', 'Actor_07', 'Actor_06', 'Actor_02', 'Actor_04', 'Actor_01', 'Actor_03']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(file_name):\n",
        "    try:\n",
        "        audio, sample_rate = librosa.load(file_name, sr=None, res_type='kaiser_fast')\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "        mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
        "        return mfccs_scaled\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_name}: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "9osKBPLk5TrI"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, Bidirectional\n",
        "\n",
        "def build_cnn_rnn_model(input_shape):\n",
        "    model = Sequential([\n",
        "        # Expand dims to add channel dimension for Conv1D\n",
        "        Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Conv1D(128, kernel_size=3, activation='relu'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Conv1D(256, kernel_size=3, activation='relu'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define input shape (features dimension, 1 channel)\n",
        "input_shape = (40, 1)\n",
        "num_classes = 8\n",
        "model = build_cnn_rnn_model(input_shape)\n"
      ],
      "metadata": {
        "id": "KIiFLdmZ5YYO"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.expand_dims(X, axis=-1)  # Add channel dimension\n"
      ],
      "metadata": {
        "id": "77H0SY6n6d89"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUhwxuNL5kEY",
        "outputId": "9a8268d0-4fdb-418e-d95c-f622f1101ccb"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.1324 - loss: 5.1981 - val_accuracy: 0.2639 - val_loss: 1.9854\n",
            "Epoch 2/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2046 - loss: 2.0621 - val_accuracy: 0.2222 - val_loss: 1.9608\n",
            "Epoch 3/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2313 - loss: 1.9970 - val_accuracy: 0.2153 - val_loss: 1.9385\n",
            "Epoch 4/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2032 - loss: 1.9842 - val_accuracy: 0.2465 - val_loss: 1.9216\n",
            "Epoch 5/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2315 - loss: 1.9733 - val_accuracy: 0.2708 - val_loss: 1.9287\n",
            "Epoch 6/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2253 - loss: 1.9442 - val_accuracy: 0.2778 - val_loss: 1.8982\n",
            "Epoch 7/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2285 - loss: 1.9431 - val_accuracy: 0.2708 - val_loss: 1.8941\n",
            "Epoch 8/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2589 - loss: 1.8946 - val_accuracy: 0.2812 - val_loss: 1.8790\n",
            "Epoch 9/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2619 - loss: 1.8807 - val_accuracy: 0.3264 - val_loss: 1.8600\n",
            "Epoch 10/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2789 - loss: 1.8653 - val_accuracy: 0.2604 - val_loss: 1.8575\n",
            "Epoch 11/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2521 - loss: 1.8649 - val_accuracy: 0.3333 - val_loss: 1.8211\n",
            "Epoch 12/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2618 - loss: 1.8360 - val_accuracy: 0.3542 - val_loss: 1.8344\n",
            "Epoch 13/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2570 - loss: 1.8480 - val_accuracy: 0.3611 - val_loss: 1.8219\n",
            "Epoch 14/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2853 - loss: 1.8452 - val_accuracy: 0.3194 - val_loss: 1.7791\n",
            "Epoch 15/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2928 - loss: 1.8346 - val_accuracy: 0.3715 - val_loss: 1.7381\n",
            "Epoch 16/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3002 - loss: 1.7909 - val_accuracy: 0.4028 - val_loss: 1.7369\n",
            "Epoch 17/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3065 - loss: 1.7885 - val_accuracy: 0.3924 - val_loss: 1.7008\n",
            "Epoch 18/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3209 - loss: 1.7640 - val_accuracy: 0.4201 - val_loss: 1.6808\n",
            "Epoch 19/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3625 - loss: 1.7213 - val_accuracy: 0.4028 - val_loss: 1.6944\n",
            "Epoch 20/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3545 - loss: 1.7115 - val_accuracy: 0.4236 - val_loss: 1.6714\n",
            "Epoch 21/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3514 - loss: 1.7139 - val_accuracy: 0.4340 - val_loss: 1.6459\n",
            "Epoch 22/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3578 - loss: 1.6764 - val_accuracy: 0.4549 - val_loss: 1.5628\n",
            "Epoch 23/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3345 - loss: 1.6830 - val_accuracy: 0.4618 - val_loss: 1.5921\n",
            "Epoch 24/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4209 - loss: 1.6020 - val_accuracy: 0.4618 - val_loss: 1.5712\n",
            "Epoch 25/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3778 - loss: 1.6044 - val_accuracy: 0.4757 - val_loss: 1.5128\n",
            "Epoch 26/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4090 - loss: 1.5939 - val_accuracy: 0.5000 - val_loss: 1.5008\n",
            "Epoch 27/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4304 - loss: 1.5528 - val_accuracy: 0.4792 - val_loss: 1.4399\n",
            "Epoch 28/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4226 - loss: 1.5441 - val_accuracy: 0.5035 - val_loss: 1.5166\n",
            "Epoch 29/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4073 - loss: 1.5369 - val_accuracy: 0.5104 - val_loss: 1.4048\n",
            "Epoch 30/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4353 - loss: 1.4731 - val_accuracy: 0.4931 - val_loss: 1.4710\n",
            "Epoch 31/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4497 - loss: 1.4914 - val_accuracy: 0.5382 - val_loss: 1.3991\n",
            "Epoch 32/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4536 - loss: 1.4483 - val_accuracy: 0.5243 - val_loss: 1.3746\n",
            "Epoch 33/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4258 - loss: 1.4528 - val_accuracy: 0.5347 - val_loss: 1.3755\n",
            "Epoch 34/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5026 - loss: 1.4163 - val_accuracy: 0.5868 - val_loss: 1.3321\n",
            "Epoch 35/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5494 - loss: 1.3016 - val_accuracy: 0.5556 - val_loss: 1.3755\n",
            "Epoch 36/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5073 - loss: 1.3820 - val_accuracy: 0.5243 - val_loss: 1.3377\n",
            "Epoch 37/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5273 - loss: 1.2737 - val_accuracy: 0.5590 - val_loss: 1.3287\n",
            "Epoch 38/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5441 - loss: 1.2641 - val_accuracy: 0.6146 - val_loss: 1.2694\n",
            "Epoch 39/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5547 - loss: 1.2957 - val_accuracy: 0.5764 - val_loss: 1.2794\n",
            "Epoch 40/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5432 - loss: 1.2122 - val_accuracy: 0.5938 - val_loss: 1.2563\n",
            "Epoch 41/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5798 - loss: 1.1879 - val_accuracy: 0.5625 - val_loss: 1.2355\n",
            "Epoch 42/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5727 - loss: 1.1685 - val_accuracy: 0.6076 - val_loss: 1.1989\n",
            "Epoch 43/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5964 - loss: 1.0856 - val_accuracy: 0.6111 - val_loss: 1.1706\n",
            "Epoch 44/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6228 - loss: 1.0658 - val_accuracy: 0.6042 - val_loss: 1.2056\n",
            "Epoch 45/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5997 - loss: 1.0717 - val_accuracy: 0.6354 - val_loss: 1.1625\n",
            "Epoch 46/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6239 - loss: 1.0173 - val_accuracy: 0.5868 - val_loss: 1.1596\n",
            "Epoch 47/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6482 - loss: 1.0048 - val_accuracy: 0.6354 - val_loss: 1.1361\n",
            "Epoch 48/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6570 - loss: 0.9491 - val_accuracy: 0.6007 - val_loss: 1.1326\n",
            "Epoch 49/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6417 - loss: 0.9740 - val_accuracy: 0.6389 - val_loss: 1.0965\n",
            "Epoch 50/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6729 - loss: 0.8998 - val_accuracy: 0.6389 - val_loss: 1.0917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define emotion labels\n",
        "emotion_labels = [\n",
        "    'Anger',    # Index 0\n",
        "    'Disgust',  # Index 1\n",
        "    'Fear',     # Index 2\n",
        "    'Happiness',# Index 3\n",
        "    'Sadness',  # Index 4\n",
        "    'Surprise', # Index 5\n",
        "    'Neutral',  # Index 6\n",
        "    'Other'     # Index 7\n",
        "]\n",
        "\n",
        "def get_emotion_label(predicted_index):\n",
        "    return emotion_labels[predicted_index]\n",
        "\n",
        "# Function to extract features\n",
        "def extract_features(file_name):\n",
        "    try:\n",
        "        audio, sample_rate = librosa.load(file_name, sr=None, res_type='kaiser_fast')\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "        mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
        "        return mfccs_scaled\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_name}: {e}\")\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "Hq5-q8BX6tv7"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_emotion(file_name, model):\n",
        "    features = extract_features(file_name)\n",
        "    if features is not None:\n",
        "        processed_features = np.expand_dims(features, axis=0)  # Add batch dimension\n",
        "        processed_features = np.expand_dims(processed_features, axis=-1)  # Add channel dimension\n",
        "        predictions = model.predict(processed_features)\n",
        "        predicted_index = np.argmax(predictions, axis=1)[0]  # Get the index of the highest probability\n",
        "\n",
        "        # Map the predicted index to an emotion label\n",
        "        predicted_emotion = get_emotion_label(predicted_index)\n",
        "        return predicted_emotion\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/drive/MyDrive/RAVIDASS/Actor_04/03-01-03-02-01-02-04.wav'\n",
        "predicted_emotion = predict_emotion(file_path, model)\n",
        "print(f\"Predicted Emotion: {predicted_emotion}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EpnABDI7fPB",
        "outputId": "3fa3d694-c6bd-4297-d84d-dcfee59c5c36"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step\n",
            "Predicted Emotion: Fear\n"
          ]
        }
      ]
    }
  ]
}